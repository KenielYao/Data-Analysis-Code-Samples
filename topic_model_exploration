Topic Modelling with text2vec in R

This data was taken from the December 15, 2016 Stack Exchange data dump\footnote{Licensed under Creative Commons Share Alike 3.0, 
https://creativecommons.org/licenses/by-sa/3.0/}. It includes two files:
\begin{align*}
&\texttt{stackexchange/20161215StatsPostsRaw.csv} \\
&\texttt{stackexchange/20161215StatsPostsMerged.csv}
\end{align*}
The cleaned file has 92,335 documents, created by combining questions and associated answers, then removing HTML, \LaTeX, code, and
stopwords. See the \texttt{README} file for further details. 

\noindent
Here is part of an entry from the cleaned up version of the collection:
\begin{align*}
&\texttt{124,``Statistical classification of text I'm a programmer without} \\
&\texttt{statistical background, and I'm currently looking at different} \\
&\texttt{classification methods for a large number of different documents that} \\
&\texttt{I want to classify into pre-defined categories. I've been reading} \\
&\texttt{about kNN, SVM and NN. However, I have some trouble getting} \\
&\texttt{started. What resources do you recommend? I do know single variable} \\
&\texttt{and multi variable calculus quite well, so my math should be strong} \\
&\texttt{enough. I also own Bishop??s book on Neural Networks, but it has proven} \\
&\texttt{to be a bit dense as an introduction. [...]}
\end{align*}

## Part a: Vocabulary Processing

First, I clean and process the data to determine a word vocabulary of size ~10,000 by requiring the minimum frequency of a word to be 45
in the corpus.


```{r, message=FALSE,warning=FALSE, eval=FALSE}
#import data
require(dplyr)
merged.post <- read.csv("~/Downloads/20161215StatsPostsMerged.csv.gz")
```

```{r, message=FALSE,warning=FALSE}
#remove numbers and symbols
require(stringr)
require(text2vec)
require(tm)
merged.post$CleanBody <- stringr::str_replace_all(merged.post$CleanBody,"[^[:alpha:]]", " ")
merged.post$CleanBody <- stringr::str_replace_all(merged.post$CleanBody,"\\s+", " ")

#remove stop words
stopwords <- c(tm::stopwords("english"))
prep_fun <- tolower
tok_fun <- word_tokenizer
tokens <- merged.post$CleanBody %>%
  prep_fun %>%
  tok_fun
it <- itoken(tokens, ids = merged.post$Id, progressbar = FALSE)

#create vocabulary
v <- create_vocabulary(it, stopwords = stopwords) %>%
  prune_vocabulary(term_count_min = 45)
#only keep words that are of length above 2
tmp <- sapply(v$term, nchar) #calculate length of each word
v.pruned <- v[tmp > 2,]

vectorizer <- vocab_vectorizer(v.pruned)

#convert to document term matrix
dtm <- create_dtm(it, vectorizer)

```


## Part b: Fitting Topic Models

Now I fit topic models on the collection. First, I divide the corpus into training and validation documents with a 90:10 split, holding
out about 9,000 documents. 

```{r, message=FALSE,warning=FALSE}
#split into training and validation documents
set.seed(1)
doc_count <- dim(merged.post)[1]
doc.split <- sample(1:doc_count, 0.9*doc_count)

train.set <- dtm[doc.split,]
test.set <- dtm[-doc.split,]

```

Next, I train topic models, using different number of topics. As a larger number of topics leads to greater specificity but often less
interpretability, I eventually settled on 30 topics. 

```{r, message=FALSE,warning=FALSE, eval=FALSE}
lda_model <- LDA$new(n_topics = 30, doc_topic_prior = 0.1, topic_word_prior = 0.01)
train_topic_dist <- 
  lda_model$fit_transform(x = train.set, n_iter = 1000, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)
```

Below are the top 10 words of each topic in decreasing order of probability:

```{r}
top_10_words <- lda_model$get_top_words(n = 10, topic_number = c(1:30), lambda = 1)
top_10_words

```

I offer an interpretation of a few of the topics above. 
Topic 4 seems highly related to PCA and linear algebra applications, given the prevalence of "matrix", "PCA" and "components"
Topic 5 seems to be related to survival analysis
Topic 6 seems related to Markov Chains
Topic 7 seems related to issues of sampling.
It seems that the top 10 words in each topic feature a lot of similar word roots, and dont feature a large amount of variation 
within each topic. It would be useful to see phrases used together e.g. "Markov Chain" instead of "Markov" and "Chain"

\noindent
I have selected several documents from the corpus below, and display barplots of their most probable topics, using their posterior
distribution. I provide a few comparisons below by manually reading the actual document, in relation to my categorisation of its topic as above.


```{r}
doc.plot <- function(doc_no) {
  barplot(train_topic_dist[doc_no, ], xlab = "topic", 
        ylab = "proportion", main=doc_no, ylim = c(0, 1), 
        names.arg = 1:ncol(train_topic_dist), las=2) #top_10_words[1,], las=2)
  #need to change names.arg to just numbers i.e. 1:ncol(train_topic_dist)

  row.names(train_topic_dist)[doc_no]
}

doc.plot(1)
```

Document 1 is largely composed of topic 13, 6, 24, and 26. 
Looking at the top 30 words in each topic, topic 13 is related to probability theory, 6 with regression, 24 with survival analysis, and
26 with clustering. Looking at the document itself, these are fairly relevant as it questions how to model an infinite Poisson arrival process. 


```{r}
doc.plot(2)
```

Topic 20 is focused on classification (particularly trees), while 3 and 25 are focused on hypothesis testing and medical topics.
Document 2 in the training set is related to ROBPCA detection in non-normal multivariate data. While highly focused on regression
methods, which may share words with classification, the other two topics do not seem too relevant.

```{r}
doc.plot(3)
```
This document is very specific with a focus on Topic 26, clustering.
The document text itself is short and solely focused on conditional probability.

Overall, from these results, it seems that while the heavily featured topics in documents have some line of connection to the document
itself, they are not clearly related. This may be due in part to the number of topics, 30, which may be too large for easy human
interpretation.

## Part c: A comment on perplexity

After the above qualitative inspection, I investigate the models more quantitatively, using perplexity. I explain the theory below:

Perplexity is strongly related to the likelihood of (the product of) held-out or unseen documents given: a) the proportions parameters,
and b) topic parameter.  This requires first conditioning on the latent variables, which requires calculating the posterior of the latent
Dirichlet allocation model based on the provided/training documents. Calculating the denominator of the posterior is computationally
intractable due to the coupling between $\theta$ and $\beta$ in determining $p_(w|\alpha, \beta)$, thus making finding the log-likelihood
and maximising it across the whole distribution computationally impossible. To tackle this, we can introduce a distribution over the
latent variables (mean-field variational inference). 
Given our log probability of observations, we can calculate an evidence lower bound and optimise (increase) it by varying the parameters of the distributions of the latent variables, where there is one parameter for each latent variable. It is the dependence of these
parameters in the true model that makes it computationally intractable. By treating them independently and then optimising each while
holding the others fixed, we can fit a lower bound close to the true value. To do so, we determine the local variables on each document
(per-document topic proportions, and per-word topic assignments), and then estimate $\alpha$ and $\beta$ of the model using these
statistics. Using the calculated $\alpha$ and $\beta$, we can then maximise the lower bound on the log likelihood of each document,
summing them up to get a log-likelihood for $p_(w|\alpha, \beta)$.
A big benefit of this method is that it can be used to stochastically optimise massive datasets (stochastic variational inference).

## Part d: Evaluating Test Set Perplexity

I evaluate the test set perplexity for a range of models, fit with $K = 10,20,...,200$ topics. A plot of test set perplexity as a
function of number of topics is provided below. 

```{r, message=FALSE, warning=FALSE, eval=FALSE}

test_topic_dist <- lda_model$transform(test.set)

perplexity(test.set, lda_model$topic_word_distribution, test_topic_dist)

```

```{r, message=FALSE,warning=FALSE, eval=FALSE}

perplexity_seq <- seq(10,200,by=10)
perplexity.plot <- data.frame(topic_number = perplexity_seq, perplexity_val = numeric(20))


for(i in 1:20) {
  model_tmp <- LDA$new(n_topics = perplexity_seq[i], doc_topic_prior = 0.1, topic_word_prior = 0.01)
  doc_topic_distr <- 
    model_tmp$fit_transform(x = train.set, n_iter = 1000, 
                            convergence_tol = 0.001, n_check_convergence = 25, 
                            progressbar = FALSE)
  
  test_topic_dist_tmp <- model_tmp$transform(test.set)

  perplexity.plot$perplexity_val[i] <- perplexity(test.set, model_tmp$topic_word_distribution, test_topic_dist_tmp)

}

```


```{r}
plot(perplexity.plot)
```

From this graph, it appears the larger the number of topics, the lower the perplexity (and hence the better). As the magnitude of the
gradient of this graph decreases with the topic_number, while increasing topic_number is beneficial, it also makes computation more
intensive. While no qualitative difference is evident, I expect that if topic_number was increased even further, we would see evidence 
of overfitting (perplexity increasing with topic_number), as while we are decreasing bias by increasing topic_number, we are also
increasing variance of the model.
