
Capital Bikeshare automates the process of obtaining membership, rental, and bike return via a network of kiosk locations throughout a city. Here, I combine historical usage patterns with weather data to forecast the company's bike rental demand in Washington, D.C.

I use hourly data spanning two years (available online at https://www.capitalbikeshare.com/system-data).
You are provided hourly rental data collected from the Capital Bikeshare system spanning two years. The file \texttt{train.txt}, as the training set, contains data for the first 19 days of each month, while \texttt{test.txt}, as the test set, contains data from the 20th to the end of the month. The dataset includes the following information:
  \begin{itemize}
\item \texttt{daylabel} - day number ranging from 1 to 731
\item \texttt{year}, \texttt{month}, \texttt{day}, \texttt{hour} - hourly date
\item \texttt{season} - 1 = spring, 2 = summer, 3 = fall, 4 = winter
\item \texttt{holiday} - whether the day is considered a holiday
\item \texttt{workingday} - whether the day is neither a weekend nor a holiday
\item \texttt{weather} - 1 = clear, few clouds, partly cloudy \\ 2 = mist + cloudy, mist + broken clouds, mist + few clouds, mist \\ 3 = light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds \\ 4 = heavy rain + ice pallets + thunderstorm + mist, snow + fog
\item \texttt{temp} - temperature in Celsius
\item \texttt{atemp} - 'feels like' temperature in Celsius
\item \texttt{humidity} - relative humidity
\item \texttt{windspeed} - wind speed
\item \texttt{count} - number of total rentals
\end{itemize}

I evaluate predictions using the root mean squared logarithmic error (RMSLE), calculated as
$$
\sqrt{\frac{1}{n}\sum_{i=1}^n(\log(m_i+1)-\log(\hat{m}_i+1))^2}
$$
where $m_i$ is the true count, $\hat{m}_i$ is the estimate, and $n$ is the number of entries to be evaluated. The RMSLE is useful to prevent over-weighting the effect of large observations over small observations, given same magnitude of error.

## Part a: Linear Model

To evaluate my models, I split training set into: i) sub-training set (first 15 days), and ii) validation set (16-19th days),

```{r}
#divide training set and transform/delete variables
bikeshare.train <- read.table("train.txt", header=TRUE)

#transform count (response variable) into log form
bikeshare.train$count <- log(bikeshare.train$count + 1)
drops <- c("casual","registered")
bikeshare.train <- bikeshare.train[,!(names(bikeshare.train) %in% drops)]
bikeshare.trainset <- bikeshare.train[bikeshare.train$day <= 15,]
bikeshare.validset <- bikeshare.train[bikeshare.train$day >=16,]

#variable types:
#factor: season; holiday; workingday; weather; month; 
#numeric: temp; atemp; humidity; windspeed; year; hour;

#best linear model
basic.lm <- lm(count~hour+humidity+factor(month)+year+factor(weather)+windspeed+workingday+
atemp, data=bikeshare.trainset)
#From hidden testing using forward, backward and step variable testing:
#>month is more predictive than season
#>working day is more predictive than holiday
#>atemp is more predictive than temp

basic.pred <- predict(basic.lm,newdata=bikeshare.validset[,-dim(bikeshare.validset)[2]])
basic.RMSLE.score <- sqrt(mean((basic.pred - bikeshare.validset$count)^2))

#RMSLE score for basic linear model
basic.RMSLE.score 

```


## Part b: Local Linear Regression

To visualise the relationship between day and the mean hourly log counts for each day:

```{r}
train.daylabel <- unique(bikeshare.trainset$daylabel)
count.mean <- unlist(lapply(1:length(train.daylabel), function(x) 
mean(bikeshare.trainset$count[bikeshare.trainset$daylabel==train.daylabel[x]],na.rm=TRUE)))
daymean.plot <- data.frame(day=train.daylabel,meancount=count.mean)

#Plot of mean hourly log counts for each day
plot(daymean.plot$meancount~daymean.plot$day)

```
As this trend is non-linear, I take a step beyond the linear model to fit local linear regressions to the means against \texttt{daylabel}.
After fitting the nonparametric curve, I use the hourly residuals as my new response variable, and fit the same model as in part (a). 


```{r}
#fit local linear regression to the mean against daylabel
loess.model <- loess(meancount~day,data=daymean.plot,span=0.25)
hourly.residuals <- loess.model$residuals
hour.residuals <- rep(hourly.residuals,each=24)

#use hourly residual as response to fit model in (a)
residual.lm <- lm(hour.residuals~hour+humidity+factor(month)+year+factor(weather)+windspeed
+workingday+atemp, data=bikeshare.trainset)
residual.pred <- predict(residual.lm,newdata=bikeshare.validset)

#calculate real residuals
valid.daylabel <- unique(bikeshare.validset$daylabel)
count.mean.valid <- unlist(lapply(1:length(valid.daylabel), function(x) 
mean(bikeshare.validset$count[bikeshare.validset$daylabel==valid.daylabel[x]],na.rm=TRUE)))
validmean.plot <- data.frame(day=valid.daylabel,meancount=count.mean.valid)
loess.valid <- loess(meancount~day,data=validmean.plot,span=0.25)
valid.df <- data.frame(daylabel=valid.daylabel,residuals=loess.valid$residuals)
valid.df2 <- bikeshare.validset[,c("daylabel","count")]
valid.final <- merge(valid.df2,valid.df,by="daylabel")

residual.RMSLE.score <- sqrt(mean((residual.pred - valid.final$residuals)^2)) 

#RMSLE score for residuals
residual.RMSLE.score 

```


## Part c: Additive Models

To extend my model, I use gam and lo functions to fit an additive model. My best result after experimental tuning is below.

```{r,message=FALSE,warning=FALSE}
library(gam)
nonp.gam <- gam(count~lo(hour,span=0.25,degree=2)+lo(humidity,span=0.4)+factor(month)+year+
factor(weather)+lo(windspeed,span=0.5)+workingday+lo(atemp,span=0.3)+lo(daylabel,span=0.25),
data=bikeshare.trainset,control = gam.control(surface = "direct"))
nonp.pred <- predict(nonp.gam,newdata=bikeshare.validset[,-dim(bikeshare.validset)[2]])
nonp.RMSLE.score <- sqrt(mean((nonp.pred - bikeshare.validset$count)^2))

#RMSLE score for additive model
nonp.RMSLE.score

```

As seen above, the additional tuning capabilities offered by the additive model allows us to decrease the RMSLE score significantly, for both training and test errors. 
